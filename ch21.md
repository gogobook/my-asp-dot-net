

Tasks and Parallel Programming
WHAT’S IN THIS CHAPTER?
An overview of multi-threading
Working with the Parallel class
Working with Tasks
Using the Cancellation framework
Using the Data Flow Library
Working with timers
Understanding threading issues
Using the lock keyword
Synchronizing with Monitor
Synchronizing with mutexes
Working with Semaphore and SemaphoreSlim
Using ManualResetEvent, AutoResetEvent, and
CountdownEvent
Working with Barrier
Managing readers and writers with ReaderWriterLockSlim
WROX.COM CODE DOWNLOADS
FOR THIS CHAPTER


The Wrox.com code downloads for this chapter are found at
www.wrox.com on the Download Code tab. The source code is also
available at
https://github.com/ProfessionalCSharp/ProfessionalCSharp7 in
the directory Tasks .
The code for this chapter is divided into the following major
examples:
Parallel
Task
Cancellation
DataFlow
Timer
WinAppTimer
ThreadingIssues
SynchronizationSamples
BarrierSample
ReaderWriterLockSample
OVERVIEW
There are several reasons for using multiple threads. Suppose that you
are making a network call from an application that might take some
time. You don’t want to stall the user interface and force the user to
wait idly until the response is returned from the server. The user could
perform some other actions in the meantime or even cancel the
request that was sent to the server. Using threads can help.
For all activities that require a wait——for example, because of file,
database, or network access——you can start a new thread to fulfill other
activities at the same time. Even if you have only processing-intensive
tasks to do, threading can help. Multiple threads of a single process
can run on different CPUs, or, nowadays, on different cores of a


multiple-core CPU, at the same time.
You must be aware of some issues when running multiple threads,
however. Because they can run during the same time, you can easily
get into problems if the threads access the same data. To avoid that,
you must implement synchronization mechanisms.
.NET offers an abstraction mechanism to threads: tasks. Tasks allow
building relations between tasks——for example, one task should
continue when the first one is completed. You can also build a
hierarchy consisting of multiple tasks.
Instead of using tasks, you can implement parallel activities using the
Parallel class. You need to differentiate data parallelism where
working with some data is processed simultaneously between different
tasks, or task parallelism where different functions are executed
simultaneously.
When creating parallel programs, you have a lot of different options.
You should use the simplest option that fits your scenario. This
chapter starts with the Parallel class that offers very easy parallelism.
If this is all you need, just use this class. In case you need more
control, such as when you need to manage a relation between tasks or
to define a method that returns a task, the Task class is the way to go.
This chapter also covers the data flow library, which might be the
easiest one to use if you need an actor-based programming to flow
data through pipelines.
In case you even need more control over parallelism, such as setting
priorities, the Thread class might be the one to use.
**NOTE**
The use of asynchronous methods with the async and await
keywords is covered in Chapter 15, “Asynchronous
Programming.”
One variant of task parallelism is offered by Parallel LINQ, which
is covered in Chapter 12, “Language Integrated Query.”


Creating a program that runs multiple tasks in parallel can lead to race
conditions and deadlocks. You need to be aware of synchronization
techniques.
It is best when you can avoid synchronization by not sharing data
between threads. Of course, this is not always possible. If data sharing
is necessary, you must use synchronization so that only one task at a
time accesses and changes the shared state. In case you don’t pay
attention to synchronization, race conditions and deadlocks can apply.
A big issue with race conditions and deadlocks is that errors can occur
from time to time. With a higher number of CPU cores, error numbers
can increase. Such errors usually are hard to find. So, it’s best to pay
attention to synchronization from the beginning.
Using multiple tasks is easy if the tasks don’t access the same
variables. You can avoid this situation to a certain degree, but at some
point, you will find some data needs to be shared. When sharing data,
you need to apply synchronization techniques. When threads access
the same data and you don’t apply synchronization, you are lucky
when the problem pops up immediately. However, this is rarely the
case. This chapter shows race conditions and deadlocks, and how you
can avoid them by applying synchronization mechanisms.
.NET offers several options for synchronization. You can use
synchronization objects within a process or across processes. You can
use them to synchronize one task or multiple tasks to access one or
more resources. Synchronization objects can also be used to inform
tasks that something completed. All these synchronization objects are
covered in this chapter.
**NOTE**
The need for synchronization can be partly avoided by using
immutable data structures as much as possible. With immutable
data structures, the data can only be initialized, but cannot be
changed afterward. That’s why synchronization is not needed


with these types.
After this long introduction, let’s start with the Parallel class——an
uncomplicated way to add parallelism to your application.
PARALLEL CLASS
One great abstraction of threads is the Parallel class. With this class,
both data and task parallelism are offered. This class is in the
namespace System.Threading.Tasks .
The Parallel class defines static methods for a parallel for and
foreach . With the C# statements for and foreach , the loop is run from
one thread. The Parallel class uses multiple tasks and, thus, multiple
threads for this job.
Whereas the Parallel.For and Parallel.ForEach methods invoke the
same code during each iteration, Parallel.Invoke enables you to
invoke different methods concurrently. Parallel.Invoke is for task
parallelism, and Parallel.ForEach is for data parallelism.
Looping with the Parallel.For Method
The Parallel.For method is like the C# for loop statement for
performing a task a number of times. With Parallel.For , the iterations
run in parallel. The order of iteration is not defined.
The sample code for ParallelSamples makes use of the following
namespaces:
Namespaces
System
System.Linq
System.Threading
System.Threading.Tasks


**NOTE**
This sample makes use of command-line arguments. To work
through the different features, pass different arguments as shown
on startup of the sample application, or by checking the Main
method. From Visual Studio, you can pass command-line
arguments in the Debug options of the project properties. Using
the dotnet command line, to pass the command-line argument -p ,
you can start the command dotnet run -- -p .
For having information about the thread and the task, the following
Log method writes thread and task identifiers to the console (code file
ParallelSamples/Program.cs ):
public static void Log(string prefix) =>
Console.WriteLine($"{prefix}, task: {Task.CurrentId}, " +
$"thread: {Thread.CurrentThread.ManagedThreadId}");
Let’s get into the Parallel.For method. With this method, the first two
parameters define the start and end of the loop. The following example
has the iterations from 0 to 9. The third parameter is an Action<int>
delegate. The integer parameter is the iteration of the loop that is
passed to the method referenced by the delegate. The return type of
Parallel.For is the struct ParallelLoopResult , which provides
information if the loop is completed:
public static void ParallelFor()
{
ParallelLoopResult result =
Parallel.For(0, 10, i =>
{
Log($"S {i}");
Task.Delay(10).Wait();
Log($"E {i}");
});
Console.WriteLine($"Is completed: {result.IsCompleted}");
}
In the body of Parallel.For , the index, task identifier, and thread


identifier are written to the console. As shown in the following output,
the order is not guaranteed. You will see different results if you run
this program once more. This run of the program had the order 2-4-0-
6-8 and so on with nine tasks and six threads. A task does not
necessarily map to one thread: a thread can be reused by different
tasks.
S 2 task: 1, thread:
S 4 task: 2, thread:
S 0 task: 4, thread:
S 6 task: 5, thread:
S 8 task: 3, thread:
E 6 task: 5, thread:
E 0 task: 4, thread:
S 1 task: 4, thread:
E 4 task: 2, thread:
S 5 task: 8, thread:
E 2 task: 1, thread:
S 9 task: 9, thread:
E 8 task: 3, thread:
S 3 task: 7, thread:
S 7 task: 6, thread:
E 5 task: 8, thread:
E 1 task: 4, thread:
E 3 task: 7, thread:
E 7 task: 6, thread:
E 9 task: 9, thread:
Is completed: True








































The delay within the parallel body waits for 10 milliseconds to have a
better chance to create new threads. If you remove this line, you see
fewer threads and tasks to be used.
What you can also see with the result is that every end-log of a loop
uses the same thread and task as the start-log. Using Task.Delay with
the Wait method blocks the current thread until the delay ends.
Change the previous example to now use the await keyword with the
Task.Delay method (code file ParallelSamples/Program.cs ):
public static void ParallelForWithAsync()
{
ParallelLoopResult result =
Parallel.For(0, 10, async i =>
{


Log($"S {i}");
await Task.Delay(10);
Log($"E {i}");
});
Console.WriteLine($"is completed: {result.IsCompleted}");
}
The result is in the following code snippet. With the output after the
Thread.Delay method you can see the thread change. For example,
loop iteration 8, which had thread ID 7 before the delay, has thread ID
5 after the delay. You can also see that tasks no longer exist——there are
only threads——and here previous threads are reused. Another
important aspect is that the For method of the Parallel class is
completed without waiting for the delay. The Parallel class waits for
the tasks it created, but it doesn’t wait for other background activity. It
is also possible that you won’t see the output from the methods after
the delay at all——if the main thread (which is a foreground thread) is
finished, all the background threads are stopped. Foreground and
background threads are discussed in the next chapter.
S 0, task: 5, thread: 1
S 8, task: 8, thread: 7
S 6, task: 7, thread: 8
S 4, task: 9, thread: 6
S 2, task: 6, thread: 5
S 7, task: 7, thread: 8
S 1, task: 5, thread: 1
S 5, task: 9, thread: 6
S 9, task: 8, thread: 7
S 3, task: 6, thread: 5
Is completed: True
E 2, task: , thread: 8
E 0, task: , thread: 8
E 8, task: , thread: 5
E 6, task: , thread: 7
E 4, task: , thread: 6
E 5, task: , thread: 7
E 7, task: , thread: 7
E 1, task: , thread: 6
E 3, task: , thread: 5
E 9, task: , thread: 8


**WARNING**
As demonstrated here, although using async features with .NET
and C# is very easy, it’s still important to know what’s happening
behind the scenes, and you have to pay attention to some issues.
Stopping Parallel.For Early
You can also break Parallel.For early without looping through all the
iterations. A method overload of the For method accepts a third
parameter of type Action<int, ParallelLoopState> . By defining a
method with these parameters, you can influence the outcome of the
loop by invoking the Break or Stop methods of the ParallelLoopState .
Remember, the order of iterations is not defined (code file
ParallelSamples/Program.cs):
public static void StopParallelForEarly()
{
ParallelLoopResult result =
Parallel.For(10, 40, (int i, ParallelLoopState pls) =>
{
Log($"S {i}");
if (i > 12)
{
pls.Break();
Log($"break now... {i}");
}
Task.Delay(10).Wait();
Log($"E {i}");
});
Console.WriteLine($"Is completed: {result.IsCompleted}");
Console.WriteLine($"lowest break iteration:
{result.LowestBreakIteration}");
}
This run of the application demonstrates that the iteration breaks up
with a value higher than 12, but other tasks can simultaneously run,
and tasks with other values can run. All the tasks that have been
started before the break can continue to the end. You can use the


LowestBreakIteration
property to ignore results from tasks that you do
not need:
S 31, task: 6, thread: 8
S 17, task: 7, thread: 5
S 10, task: 5, thread: 1
S 24, task: 8, thread: 6
break now 24, task: 8, thread:
S 38, task: 9, thread: 7
break now 38, task: 9, thread:
break now 31, task: 6, thread:
break now 17, task: 7, thread:
E 17, task: 7, thread: 5
E 10, task: 5, thread: 1
S 11, task: 5, thread: 1
E 38, task: 9, thread: 7
E 24, task: 8, thread: 6
E 31, task: 6, thread: 8
E 11, task: 5, thread: 1
S 12, task: 5, thread: 1
E 12, task: 5, thread: 1
S 13, task: 5, thread: 1
break now 13, task: 5, thread:
E 13, task: 5, thread: 1
Is completed: False
lowest break iteration: 13










Parallel For Initialization
might use several threads to do the loops. If you need an
initialization that should be done with every thread, you can use the
Parallel.For<TLocal> method. The generic version of the For method
accepts——in addition to the from and to values——three delegate
parameters. The first parameter is of type Func<TLocal> . Because the
example here uses a string for TLocal , the method needs to be defined
as Func<string> , a method returning a string . This method is invoked
only once for each thread that is used to do the iterations.
Parallel.For
The second delegate parameter defines the delegate for the body. In
the example, the parameter is of type Func<int, ParallelLoopState,
string, string> . The first parameter is the loop iteration; the second
parameter, ParallelLoopState , enables stopping the loop, as shown
earlier. With the third parameter, the body method receives the value


that is returned from the init method. The body method also needs to
return a value of the type that was defined with the generic For
parameter.
The last parameter of the For method specifies a delegate,
Action<TLocal>; in the example, a string is received. This method, a
thread exit method, is called only once for each thread (code file
ParallelSamples/Program.cs):
public static void ParallelForWithInit()
{
Parallel.For<string>(0, 10, () =>
{
// invoked once for each thread
Log($"init thread");
return $"t{Thread.CurrentThread.ManagedThreadId}";
},
(i, pls, str1) =>
{
// invoked for each member
Log($"body i {i} str1 {str1}");
Task.Delay(10).Wait();
return $"i {i}";
},
(str1) =>
{
// final action on each thread
Log($"finally {str1}");
});
}
The result of running this program once is shown here:
init thread task: 7, thread: 6
init thread task: 6, thread: 5
body i: 4 str1: t6 task: 7, thread: 6
body i: 2 str1: t5 task: 6, thread: 5
init thread task: 5, thread: 1
body i: 0 str1: t1 task: 5, thread: 1
init thread task: 9, thread: 8
body i: 8 str1: t8 task: 9, thread: 8
init thread task: 8, thread: 7
body i: 6 str1: t7 task: 8, thread: 7
body i: 1 str1: i 0 task: 5, thread: 1
finally i 2 task: 6, thread: 5
init thread task: 16, thread: 5


finally i 8 task: 9, thread: 8
init thread task: 17, thread: 8
body i: 9 str1: t8 task: 17, thread: 8
finally i 6 task: 8, thread: 7
init thread task: 18, thread: 7
body i: 7 str1: t7 task: 18, thread: 7
finally i 4 task: 7, thread: 6
init thread task: 15, thread: 10
body i: 3 str1: t10 task: 15, thread: 10
body i: 5 str1: t5 task: 16, thread: 5
finally i 1 task: 5, thread: 1
finally i 5 task: 16, thread: 5
finally i 3 task: 15, thread: 10
finally i 7 task: 18, thread: 7
finally i 9 task: 17, thread: 8
The output shows that the init method is called only once for each
thread; the body of the loop receives the first string from the
initialization and passes this string to the next iteration of the body
with the same thread. Lastly, the final action is invoked once for each
thread and receives the last result from every body.
With this functionality, this method fits perfectly to accumulate a
result of a huge data collection.
Looping with the Parallel.ForEach Method
Parallel.ForEach iterates through a collection implementing
IEnumerable in a way like the foreach statement, but in an
asynchronous manner. Again, the order is not guaranteed (code file
ParallelSamples/Program.cs ):
public static void ParallelForEach()
{
string[] data = {"zero", "one", "two", "three", "four",
"five",
"six", "seven", "eight", "nine", "ten", "eleven",
"twelve"};
ParallelLoopResult result =
Parallel.ForEach<string>(data, s =>
{
Console.WriteLine(s);
});
}


If you need to break up the loop, you can use an overload of the
ForEach method with a ParallelLoopState parameter. You can do this
in the same way you did earlier with the For method. An overload of
the ForEach method can also be used to access an indexer to get the
iteration number, as shown here:
Parallel.ForEach<string>(data, (s, pls, l) =>
{
Console.WriteLine($"{s} {l}");
});
Invoking Multiple Methods with the Parallel.Invoke
Method
If multiple tasks should run in parallel, you can use the
Parallel.Invoke method, which offers the task parallelism pattern.
Parallel.Invoke allows the passing of an array of Action delegates,
whereby you can assign methods that should run. The example code
passes the Foo and Bar methods to be invoked in parallel (code file
ParallelSamples/Program.cs ):
public static void ParallelInvoke()
{
Parallel.Invoke(Foo, Bar);
}
public static void Foo() =>
Console.WriteLine("foo");
public static void Bar() =>
Console.WriteLine("bar");
The Parallel class is very easy to use——for both task and data
parallelism. If more control is needed, and you don’t want to wait until
the action started with the Parallel class is completed, the Task class
comes in handy. Of course, it’s also possible to combine the Task and
Parallel classes.
TASKS
For more control over the parallel actions, you can use the Task class


from the namespace System.Threading.Tasks . A task represents some
unit of work that should be done. This unit of work can run in a
separate thread, and it is also possible to start a task in a synchronized
manner, which results in a wait for the calling thread. With tasks, you
have an abstraction layer but also a lot of control over the underlying
threads.
Tasks provide much more flexibility in organizing the work you need
to do. For example, you can define continuation work——what should be
done after a task is complete. This can be differentiated based on
whether the task was successful. You can also organize tasks in a
hierarchy. For example, a parent task can create new children tasks.
Optionally, this can create a dependency, so canceling a parent task
also cancels its child tasks.
Starting Tasks
To start a task, you can use either the TaskFactory or the constructor of
the Task and the Start method. The Task constructor gives you more
flexibility in creating the task.
The sample code for TaskSamples makes use of the following
namespaces:
Namespaces
System
System.Linq
System.Threading
System.Threading.Tasks
When starting a task, an instance of the Task class can be created, and
the code that should run can be assigned with an Action or
Action<object> delegate, with either no parameters or one object
parameter. In the following example, a method is defined with one
parameter: TaskMethod . The implementation invokes the Log method
where the ID of the task and the ID of the thread are written to the
console, as well as information if the thread is coming from a thread


pool, and if the thread is a background thread. Writing multiple
messages to the console is synchronized by using the lock keyword
with the s_logLock synchronization object. This way, parallel calls to
Log can be done, and multiple writes to the console are not
interleaving each other. Otherwise the title could be written by one
task, and the thread information follows by another task (code file
TaskSamples/Program.cs ):
public static void TaskMethod(object o)
{
Log(o?.ToString());
}
private static object s_logLock = new object();
public static void Log(string title)
{
lock (s_logLock)
{
Console.WriteLine(title);
Console.WriteLine($"Task id: {Task.CurrentId?.ToString()
?? "no task"}, " +
$"thread: {Thread.CurrentThread.ManagedThreadId}");
Console.WriteLine($"is pooled thread: " +
$"{Thread.CurrentThread.IsThreadPoolThread}");
Console.WriteLine($"is background thread: " +
$"{Thread.CurrentThread.IsBackground}");
Console.WriteLine();
}
}
The following sections describe different ways to start a new task.
Tasks Using the Thread Pool
In this section, diverse ways are shown to start a task that uses a
thread from the thread pool. The thread pool offers a pool of
background threads. The thread pool manages threads on its own,
increasing or decreasing the number of threads within the pool as
needed. Threads from the pool are used to fulfill some actions and
returned to the pool afterward.
The first way to create a task is with an instantiated TaskFactory ,
where the method TaskMethod is passed to the StartNew method, and
the task is immediately started. The second approach uses the static


property of the Task class to get access to the TaskFactory , and
to invoke the StartNew method. This is very similar to the first version
in that it uses a factory, but there’s less control over factory creation.
The third approach uses the constructor of the Task class. When the
Task object is instantiated, the task does not run immediately. Instead,
it is given the status Created . The task is then started by calling the
Start method of the Task class. The fourth approach calls the Run
method of the Task that immediately starts the task. The Run method
doesn’t have an overloaded variant to pass an Action<object> delegate,
but it’s easy to simulate this by assigning a lambda expression of type
Action , and using the parameter within its implementation (code file
TaskSamples/Program.cs ):
Factory
public void TasksUsingThreadPool()
{
var tf = new TaskFactory();
Task t1 = tf.StartNew(TaskMethod, "using a task factory");
Task t2 = Task.Factory.StartNew(TaskMethod, "factory via a
task");
var t3 = new Task(TaskMethod, "using a task constructor and
Start");
t3.Start();
Task t4 = Task.Run(() => TaskMethod("using the Run
method"));
}
The output returned with these variants is as follows. All these
versions create a new task, and a thread from the thread pool is used:
using a task factory
Task id: 1, thread: 4
is pooled thread: True
is background thread: True
factory via a task
Task id: 2, thread: 3
is pooled thread: True
is background thread: True
using a task constructor and Start
Task id: 3, thread: 5
is pooled thread: True
is background thread: True


using the Run method
Task id: 4, thread: 6
is pooled thread: True
is background thread: True
With both the Task constructor and the StartNew method of the
TaskFactory , you can pass values from the enumeration
TaskCreationOptions . Using this creation option, you can change how
the task should behave differently, as is shown in the next sections.
Synchronous Tasks
A task does not necessarily mean to use a thread from a thread pool——it
can use other threads as well. Tasks can also run synchronously, with
the same thread as the calling thread. The following code snippet uses
the method RunSynchronously of the Task class (code file
TaskSamples/Program.cs ):
private static void RunSynchronousTask()
{
TaskMethod("just the main thread");
var t1 = new Task(TaskMethod, "run sync");
t1.RunSynchronously();
}
Here, the TaskMethod is first called directly from the main thread
before it is invoked from the newly created Task . As you can see from
the following console output, the main thread doesn’t have a task ID.
It is not a pooled thread. Calling the method RunSynchronously uses the
same thread as the calling thread, but creates a task if one wasn’t
created previously:
just the main thread
Task id: no task, thread: 2
is pooled thread: False
is background thread: False
run sync
Task id: 1, thread: 2
is pooled thread: False
is background thread: False
Tasks Using a Separate Thread


If the code of a task should run for a longer time, you should use
TaskCreationOptions.LongRunning to instruct the task scheduler to
create a new thread, rather than use a thread from the thread pool.
This way, the thread doesn’t need to be managed by the thread pool.
When a thread is taken from the thread pool, the task scheduler can
decide to wait for an already running task to be completed and use this
thread instead of creating a new thread with the pool. With a long-
running thread, the task scheduler knows immediately that it doesn’t
make sense to wait for this one. The following code snippet creates a
long-running task (code file TaskSamples/Program.cs ):
private static void LongRunningTask()
{
var t1 = new Task(TaskMethod, "long running",
TaskCreationOptions.LongRunning);
t1.Start();
}
Indeed, using the option TaskCreationOptions.LongRunning , a thread
from the thread pool is not used. Instead, a new thread is created:
long running
Task id: 5, thread: 7
is pooled thread: False
is background thread: True
Futures——Results from Tasks
When a task is finished, it can write some stateful information to a
shared object. Such a shared object must be thread-safe. Another
option is to use a task that returns a result. Such a task is also known
as future as it returns a result in the future. With early versions of the
Task Parallel Library (TPL), the class had the name Future as well.
Now it is a generic version of the Task class. With this class it is
possible to define the type of the result that is returned with a task.
A method that is invoked by a task to return a result can be declared
with any return type. The following example method TaskWithResult
returns two int values with the help of a tuple. The input of the
method can be void or of type object , as shown here (code file
TaskSamples/Program.cs ):


public static (int Result, int Remainder)
TaskWithResult(object division)
{
(int x, int y) = ((int x, int y))division;
int result = x / y;
int remainder = x % y;
Console.WriteLine("task creates a result...");
return (result, remainder);
}
**NOTE**
Tuples allow you to combine multiple values into one. Tuples are
explained in Chapter 13, “Functional Programming with C#.”
When you define a task to invoke the method TaskWithResult , you use
the generic class Task<TResult> . The generic parameter defines the
return type. With the constructor, the method is passed to the Func
delegate, and the second parameter defines the input value. Because
this task needs two input values in the object parameter, a tuple is
created as well. Next, the task is started. The Result property of the
Task instance t1 blocks and waits until the task is completed. Upon
task completion, the Result property contains the result from the task:
public static void TaskWithResultDemo()
{
var t1 = new Task<(int Result, int Remainder)>
(TaskWithResult, (8, 3));
t1.Start();
Console.WriteLine(t1.Result);
t1.Wait();
Console.WriteLine($"result from task: {t1.Result.Result} "
+
$"{t1.Result.Remainder}");
}
Continuation Tasks
With tasks, you can specify that after a task is finished another specific
task should start to run——for example, a new task that uses a result


from the previous one or should do some cleanup if the previous task
failed.
Whereas the task handler has either no parameter or one object
parameter, the continuation handler has a parameter of type Task .
Here, you can access information about the originating task (code file
TaskSamples/Program.cs ):
private static void DoOnFirst()
{
Console.WriteLine($"doing some task {Task.CurrentId}");
Task.Delay(3000).Wait();
}
private static void DoOnSecond(Task t)
{
Console.WriteLine($"task {t.Id} finished");
Console.WriteLine($"this task id {Task.CurrentId}");
Console.WriteLine("do some cleanup");
Task.Delay(3000).Wait();
}
A continuation task is defined by invoking the ContinueWith method on
a task. You could also use the TaskFactory for this.
t1.OnContinueWith(DoOnSecond) means that a new task invoking the
method DoOnSecond should be started as soon as the task t1 is finished.
You can start multiple tasks when one task is finished, and a
continuation task can have another continuation task, as this next
example demonstrates (code file TaskSamples/Program.cs ):
public static void ContinuationTasks()
{
Task t1 = new Task(DoOnFirst);
Task t2 = t1.ContinueWith(DoOnSecond);
Task t3 = t1.ContinueWith(DoOnSecond);
Task t4 = t2.ContinueWith(DoOnSecond);
t1.Start();
}
So far, the continuation tasks have been started when the previous
task was finished, regardless of the result. With values from
TaskContinuationOptions , you can define that a continuation task
should only start if the originating task was successful (or faulted).


Some of the possible values are OnlyOnFaulted , NotOnFaulted ,
OnlyOnCanceled , NotOnCanceled , and OnlyOnRanToCompletion :
Task t5 = t1.ContinueWith(DoOnError,
TaskContinuationOptions.OnlyOnFaulted);
**NOTE**
The compiler-generated code from the await keyword discussed in
Chapter 15 makes use of continuation tasks.
Task Hierarchies
With task continuations, one task is started after another. Tasks can
also form a hierarchy. When a task starts a new task, a parent/child
hierarchy is started.
In the code snippet that follows, within the task of the parent, a new
task object is created, and the task is started. The code to create a child
task is the same as that to create a parent task. The only difference is
that the task is created from within another task (code file
TaskSamples/Program.cs ):
public static void ParentAndChild()
{
var parent = new Task(ParentTask);
parent.Start();
Task.Delay(2000).Wait();
Console.WriteLine(parent.Status);
Task.Delay(4000).Wait();
Console.WriteLine(parent.Status);
}
private static void ParentTask()
{
Console.WriteLine($"task id {Task.CurrentId}");
var child = new Task(ChildTask);
child.Start();
Task.Delay(1000).Wait();
Console.WriteLine("parent started child");
}


private static void ChildTask()
{
Console.WriteLine("child");
Task.Delay(5000).Wait();
Console.WriteLine("child finished");
}
If the parent task is finished before the child task, the status of the
parent task is shown as WaitingForChildrenToComplete . The parent
task is completed with the status RanToCompletion as soon as all
children tasks are completed as well. Of course, this is not the case if
the parent creates a task with the TaskCreationOption
DetachedFromParent .
Canceling a parent task also cancels the children. The cancellation
framework is discussed next.
Returning Tasks from Methods
A method that returns a task with results is declared to return Task<T>
——for example, a method that returns a task with a collection of strings:
public Task<IEnumerable<string>> TaskMethodAsync()
{
}
Creating methods that access the network or data access are often
asynchronous, with such a result so you can use task features to deal
with the results (for example, by using the async keyword as explained
in Chapter 15). In case you have a synchronous path or need to
implement an interface that is defined that way with synchronous
code, there’s no need to create a task for the sake of the result value.
The Task class offers creating a result with a completed task that is
finished with the status RanToCompletion using the method FromResult :
return Task.FromResult<IEnumerable<string>>(
new List<string>() { "one", "two" });
Waiting for Tasks
Probably you’ve already seen the WhenAll and WaitAll methods of the


class and wondered what the difference might be. Both methods
wait for all tasks that are passed to these methods to complete. The
WaitAll method blocks the calling task until all tasks that are waited
for are completed. The WhenAll method returns a task which in turn
allows you to use the async keyword to wait for the result, and it does
not block the waiting task.
Task
Although the WhenAll and WaitAll methods are finished when all the
tasks you are waiting for are completed, you can wait for just one task
of a list to be completed with WhenAny and WaitAny . Like the WhenAll
and WaitAll methods, the WaitAny method blocks the calling task,
whereas WhenAny returns a task that can be awaited.
A method that already has been used several times with several
samples is the Task.Delay method. You can specify a number of
milliseconds to wait before the task that is returned from this method
is completed.
In case all that should be done is to give up the CPU and thus allow
other tasks to run, you can invoke the Task.Yield method. This
method gives up the CPU and lets other tasks run. In case no other
task is waiting to run, the task calling Task.Yield continues
immediately. Otherwise it needs to wait until the CPU is scheduled
again for the calling task.
Value Tasks
In case a method sometimes runs asynchronously, but not always, the
Task class might be some overhead that’s not needed. .NET now offers
ValueTask that is a struct compared to the Task that is a class, thus the
ValueTask doesn’t have the overhead of an object in the heap. Usually
invoking asynchronous methods, such as making calls to an API server
or a database, the overhead of the Task type can be ignored compared
to the time needed for the work to be done. However, there are some
cases where the overhead cannot be ignored, such as when a method is
called thousands of times, and it rarely really needs a call across the
network. This is a scenario where the ValueTask becomes handy.
Let’s get into an example. The method GetTheRealData simulates a
method that usually takes a long time, accessing data from the


network or a database. Here, sample data is generated with the
Enumerable class. Together with the time, the data is retrieved, a result
in the form of a tuple is returned. This method returns a Task as we are
used to (code file ValueTaskSample/Program.cs ):
public static Task<(IEnumerable<string> data, DateTime
retrievedTime)>
GetTheRealData() =>
Task.FromResult(
(Enumerable.Range(0, 10)
.Select(x => $"item {x}").AsEnumerable(),
DateTime.Now));
The interesting part now follows in the method GetSomeData . This
method is declared to return a ValueTask . With the implementation,
first a check is done if cached data is not older than 5 seconds. If the
cached data is not older, the cached data is directly returned and
passed to the ValueTask constructor. This doesn’t really need a
background thread; the data can be directly returned. In case the
cache is older, the GetTheRealData method is invoked. This method
needs a real task and could occur with some delay (code file
ValueTaskSample/Program.cs ):
private static DateTime _retrieved;
private static IEnumerable<string> _cachedData;
public static async ValueTask<IEnumerable<string>>
GetSomeDataAsync()
{
if (_retrieved >= DateTime.Now.AddSeconds(-5))
{
Console.WriteLine("data from the cache");
return await new ValueTask<IEnumerable<string>>
(_cachedData);
}
Console.WriteLine("data from the service");
(_cachedData, _retrieved) = await GetTheRealData();
return _cachedData;
}
**NOTE**


The constructor of the ValueTask accepts type TResult for the data
to be returned, or Task<TResult> to supply a Task returned from
methods that do run asynchronously.
The Main method includes a loop to invoke the GetSomeDataAsync
method several times with a delay after every iteration (code file
ValueTaskSample/Program.cs ):
static async Task Main(string[] args)
{
for (int i = 0; i < 20; i++)
{
IEnumerable<string> data = await GetSomeDataAsync();
await Task.Delay(1000);
}
Console.ReadLine();
}
When you run the application, you can see that the data is returned
from the cache, and after the cache is invalidated, the service is
accessed first before the cache is used again.
data
data
data
data
data
data
data
data
data
data
data
data
...
from
from
from
from
from
from
from
from
from
from
from
from
the
the
the
the
the
the
the
the
the
the
the
the
service
cache
cache
cache
cache
service
cache
cache
cache
cache
service
cache
**NOTE**
Probably you don’t have scenarios yet where you can’t ignore the
overhead from tasks compared to value tasks. However, having
this core feature in the framework allows for upcoming features


such as async streams or async operators in a future C# version.
CANCELLATION FRAMEWORK
.NET includes a cancellation framework to enable the canceling of
long-running tasks in a standard manner. Every blocking call should
support this mechanism. Of course, not every blocking call currently
implements this new technology, but more and more are doing so.
Among the technologies that offer this mechanism already are tasks,
concurrent collection classes, and Parallel LINQ, as well as several
synchronization mechanisms.
The cancellation framework is based on cooperative behavior; it is not
forceful. A long-running task checks whether it is canceled and returns
control accordingly.
A method that supports cancellation accepts a CancellationToken
parameter. This class defines the property IsCancellationRequested ,
whereby a long operation can check to see whether it should abort.
Other ways for a long operation to check for cancellation include using
a WaitHandle property that is signaled when the token is canceled or
using the Register method. The Register method accepts parameters
of type Action and ICancelableOperation . The method that is
referenced by the Action delegate is invoked when the token is
canceled. This is like the ICancelableOperation , whereby the Cancel
method of an object implementing this interface is invoked when the
cancellation is done.
The sample code for cancellation samples makes use of the following
namespaces:
Namespaces
System
System.Threading
System.Threading.Tasks
Cancellation of Parallel.For


This section starts with a simple example using the Parallel.For
method. The Parallel class provides overloads for the For method,
whereby you can pass a parameter of type ParallelOptions . With
ParallelOptions , you can pass a CancellationToken . The
CancellationToken is generated by creating a CancellationTokenSource .
CancellationTokenSource implements the interface
ICancelableOperation and can therefore be registered with the
CancellationToken and allows cancellation with the Cancel method.
The example doesn’t call the Cancel method directly but uses the
CancelAfter method to cancel the token after 500 milliseconds.
Within the implementation of the For loop, the Parallel class verifies
the outcome of the CancellationToken and cancels the operation. Upon
cancellation, the For method throws an exception of type
OperationCanceledException , which is caught in the example. With the
CancellationToken , it is possible to register for information when the
cancellation is done. This is accomplished by calling the Register
method and passing a delegate that is invoked on cancellation (code
file CancellationSamples/Program.cs ):
public static void CancelParallelFor()
{
var cts = new CancellationTokenSource();
cts.Token.Register(() => Console.WriteLine("*** token
cancelled"));
// send a cancel after 500 ms
cts.CancelAfter(500);
try
{
ParallelLoopResult result =
Parallel.For(0, 100, new ParallelOptions
{
CancellationToken = cts.Token,
},
x =>
{
Console.WriteLine($"loop {x} started");
int sum = 0;
for (int i = 0; i < 100; i++)
{
Task.Delay(2).Wait();
sum += i;
}


Console.WriteLine($"loop {x} finished");
});
}
catch (OperationCanceledException ex)
{
Console.WriteLine(ex.Message);
}
}
When you run the application, you get output like the following.
Iteration 0, 50, 25, 75, and 1 were all started. This is on a system with a
quad-core CPU. With the cancellation, all other iterations were
canceled before starting. The iterations that were started are allowed
to finish because cancellation is always done in a cooperative way to
avoid the risk of resource leaks when iterations are canceled
somewhere in between:
loop 0 started
loop 50 started
loop 25 started
loop 75 started
loop 1 started
*** token cancelled
loop 75 finished
loop 50 finished
loop 1 finished
loop 0 finished
loop 25 finished
The operation was canceled.
Cancellation of Tasks
The same cancellation pattern is used with tasks. First, a new
CancellationTokenSource is created. If you need just one cancellation
token, you can use a default token by accessing
Task.Factory.CancellationToken . Then, like the previous code, the
task is canceled after 500 milliseconds. The task doing the major work
within a loop receives the cancellation token via the TaskFactory
object. The cancellation token is assigned to the TaskFactory by setting
it in the constructor. This cancellation token is used by the task to
check whether cancellation is requested by checking the
IsCancellationRequested property of the CancellationToken (code file


CancellationSamples/Program.cs ):
public void CancelTask()
{
var cts = new CancellationTokenSource();
cts.Token.Register(() => Console.WriteLine("*** task
cancelled"));
// send a cancel after 500 ms
cts.CancelAfter(500);
Task t1 = Task.Run(() =>
{
Console.WriteLine("in task");
for (int i = 0; i < 20; i++)
{
Task.Delay(100).Wait();
CancellationToken token = cts.Token;
if (token.IsCancellationRequested)
{
Console.WriteLine("cancelling was requested, " +
"cancelling from within the task");
token.ThrowIfCancellationRequested();
break;
}
Console.WriteLine("in loop");
}
Console.WriteLine("task finished without cancellation");
}, cts.Token);
try
{
t1.Wait();
}
catch (AggregateException ex)
{
Console.WriteLine($"exception: {ex.GetType().Name},
{ex.Message}");
foreach (var innerException in ex.InnerExceptions)
{
Console.WriteLine($"inner exception:
{ex.InnerException.GetType()}," +
$"{ex.InnerException.Message}");
}
}
}
When you run the application, you can see that the task starts, runs for
a few loops, and gets the cancellation request. The task is canceled and


throws a TaskCanceledException , which is initiated from the method
call ThrowIfCancellationRequested . With the caller waiting for the
task, you can see that the exception AggregateException is caught and
contains the inner exception TaskCanceledException . This is used for a
hierarchy of cancellations——for example, if you run a Parallel.For
within a task that is canceled as well. The final status of the task is
Canceled :
in task
in loop
in loop
in loop
in loop
*** task cancelled
cancelling was requested, cancelling from within the task
exception: AggregateException, One or more errors occurred.
inner exception: TaskCanceledException, A task was canceled.
DATA FLOW
The Parallel and Task classes, and Parallel LINQ, help a lot with data
parallelism. However, these classes do not directly support dealing
with data flow or transforming data in parallel. For this, you can use
Task Parallel Library Data Flow, or TPL Data Flow.
The sample code for the data flow samples makes use of the following
namespaces:
Namespaces
System
System.IO
System.Threading
System.Threading.Tasks
System.Threading.Tasks.DataFlow
Using an Action Block
The heart of TPL Data Flow is data blocks. These blocks can act as a


source to offer some data or a target to receive data, or both. Let’s start
with a simple example, a data block that receives some data and writes
it to the console. The following code snippet defines an ActionBlock
that receives a string and writes information to the console. The Main
method reads user input within a while loop, and posts every string
read to the ActionBlock by calling the Post method. The Post method
posts an item to the ActionBlock , which deals with the message
asynchronously, writing the information to the console (code file
SimpleDataFlowSample/Program.cs ):
static void Main()
{
var processInput = new ActionBlock<string>(s =>
{
Console.WriteLine($"user input: {s}");
});
bool exit = false;
while (!exit)
{
string input = ReadLine();
if (string.Compare(input, "exit", ignoreCase: true) == 0)
{
exit = true;
}
else
{
processInput.Post(input);
}
}
}
Source and Target Blocks
When the method assigned to the ActionBlock from the previous
example executes, the ActionBlock uses a task to do the execution in
parallel. You could verify this by checking the task and thread
identifiers and writing these to the console. Every block implements
the interface IDataflowBlock , which contains the property Completion ,
which returns a Task , and the methods Complete and Fault . Invoking
the Complete method, the block no longer accepts any input or
produces any more output. Invoking the Fault method puts the block
into a faulting state.


As mentioned earlier, a block can be either a source or a target, or
both. In this case, the ActionBlock is a target block and thus
implements the interface ITargetBlock . ITargetBlock derives from
IDataflowBlock and defines the OfferMessage method, in addition to
the members of the IDataBlock interface. OfferMessage sends a
message that can be consumed by the block. An API that is easier to
use than OfferMessage is the Post method, which is implemented as an
extension method for the ITargetBlock interface. The Post method was
also used by the sample application.
The ISourceBlock interface is implemented by blocks that can act as a
data source. ISourceBlock offers methods in addition to the members
of the IDataBlock interface to link to a target block and to consume
messages.
The BufferBlock acts as both a source and a target, implementing both
ISourceBlock and ITargetBlock . In the next example, this BufferBlock
is used to both post messages and receive messages (code file
SimpleDataFlowSample/Program.cs ):
The Producer method reads strings from the console and writes them
to the BufferBlock by invoking the Post method:
public static void Producer()
{
bool exit = false;
while (!exit)
{
string input = ReadLine();
if (string.Compare(input, "exit", ignoreCase: true) == 0)
{
exit = true;
}
else
{
s_buffer.Post(input);
}
}
The Consumer method contains a loop to receive data from the
BufferBlock by invoking the ReceiveAsync method. ReceiveAsync is an
extension method for the ISourceBlock interface:


public static async Task ConsumerAsync()
{
while (true)
{
string data = await s_buffer.ReceiveAsync();
Console.WriteLine($"user input: {data}");
}
}
Now, you just need to start the producer and consumer. You do this
with two independent tasks in the Main method:
static void Main()
{
Task t1 = Task.Run(() => Producer());
Task t2 = Task.Run(async () => await ConsumerAsync());
Task.WaitAll(t1, t2);
}
When you run the application, the producer task reads data from the
console, and the consumer receives the data to write it to the console.
Connecting Blocks
This section creates a pipeline by connecting multiple blocks. First,
three methods are created that will be used by the blocks. The
GetFileNames method receives a directory path and yields the
filenames that end with the . cs extension (code file
DataFlowSample/Program.cs ):
public static IEnumerable<string> GetFileNames(string path)
{
foreach (var fileName in Directory.EnumerateFiles(path,
"*.cs"))
{
yield return fileName;
}
}
The LoadLines method receives a list of filenames and yields every line
of the files:
public static IEnumerable<string>
LoadLines(IEnumerable<string> fileNames)
{


foreach (var fileName in fileNames)
{
using (FileStream stream = File.OpenRead(fileName))
{
var reader = new StreamReader(stream);
string line = null;
while ((line = reader.ReadLine()) != null)
{
//WriteLine($"LoadLines {line}");
yield return line;
}
}
}
}
The third method, GetWords , receives the lines collection and splits it
up line by line to yield return a list of words:
public static IEnumerable<string>
GetWords(IEnumerable<string> lines)
{
foreach (var line in lines)
{
string[] words = line.Split(' ', ';', '(', ')', '{', '}',
'.', ',');
foreach (var word in words)
{
if (!string.IsNullOrEmpty(word))
yield return word;
}
}
}
To create the pipeline, the SetupPipeline method creates three
TransformBlock objects. The TransformBlock is a source and target
block that transforms the source by using a delegate. The first
TransformBlock is declared to transform a string to
IEnumerable<string> . The transformation is done by the GetFileNames
method that is invoked within the lambda expression passed to the
constructor of the first block. Similarly, the next two TransformBlock
objects are used to invoke the LoadLines and GetWords methods:
public static ITargetBlock<string> SetupPipeline()
{
var fileNamesForPath = new TransformBlock<string,


IEnumerable<string>>(
path => GetFileNames(path));
var lines = new TransformBlock<IEnumerable<string>,
IEnumerable<string>>(
fileNames => LoadLines(fileNames));
var words = new TransformBlock<IEnumerable<string>,
IEnumerable<string>>(
lines2 => GetWords(lines2));
The last block defined is an ActionBlock . This block has been used
before and is just a target block to receive data:
var display = new ActionBlock<IEnumerable<string>>(
coll =>
{
foreach (var s in coll)
{
Console.WriteLine(s);
}
});
Finally, the blocks are connected to each other. fileNamesForPath is
linked to the lines block. The result from fileNamesForPath is passed
to the lines block. The lines block links to the words block, and the
words block links to the display block. Last, the block to start the
pipeline is returned:
fileNamesForPath.LinkTo(lines);
lines.LinkTo(words);
words.LinkTo(display);
return fileNamesForPath;
}
The Main method now needs to kick off the pipeline. Invoking the Post
method to pass a directory, the pipeline starts and finally writes words
from the C# source code to the console. Here, it would be possible to
start multiple requests for the pipeline, passing more than one
directory, and doing these tasks in parallel:
static void Main()
{
var target = SetupPipeline();
target.Post(".");


Console.ReadLine();
}
With this brief introduction to the TPL Data Flow library, you’ve seen
the principal way to work with this technology. This library offers a lot
more functionality, such as different blocks that deal with data
differently. The BroadcastBlock allows passing the input source to
multiple targets (for example, writing data to a file and displaying it),
the JoinBlock joins multiple sources to one target, and the BatchBlock
batches input into arrays. Using DataflowBlockOptions options allows
configuration of a block, such as the maximum number of items that
are processed within a single task and passing a cancellation token
that allows canceling a pipeline. With links, you can also filter
messages and only pass messages that fulfill a specified predicate.
TIMERS
With a timer, you can do a repeat invocation of a method. Two timers
will be covered in this section: the Timer class from the
System.Threading namespace and the DispatcherTimer for XAML-
based apps.
Using the System.Threading.Timer class, you can pass the method to be
invoked as the first parameter in the constructor. This method must
fulfill the requirements of the TimerCallback delegate, which defines a
void return type and an object parameter. With the second parameter
of the constructor, you can pass any object, which is then received with
the object argument in the callback method. For example, you can pass
an Event object to signal the caller. The third parameter specifies the
time span during which the callback should be invoked the first time.
With the last parameter, you specify the repeating interval for the
callback. If the timer should fire only once, set the fourth parameter to
the value –1 .
If the time interval should be changed after creating the Timer object,
you can pass new values with the Change method (code file
TimerSample/Program.cs ):
private static void ThreadingTimer()
{


void TimeAction(object o) =>
Console.WriteLine($"System.Threading.Timer
{DateTime.Now:T}");
using (var t1 = new Timer(TimeAction, null,
TimeSpan.FromSeconds(2), TimeSpan.FromSeconds(3)))
{
Task.Delay(15000).Wait();
}
}
The DispatcherTimer from the namespace Windows.UI.Xaml (for UWP
apps) is a timer for XAML-based apps where the event handler is
called within the UI thread, thus it is possible to directly access user
interface elements.
The sample application to demonstrate the DispatcherTimer is a
Windows app that shows the hand of a clock that switches every
second. The following XAML code defines the commands that enable
you to start and stop the clock (code file WinAppTimer/MainPage.xaml ):
<Page.TopAppBar>
<CommandBar IsOpen="True">
<AppBarButton Icon="Play" Click="{x:Bind OnTimer}" />
<AppBarButton Icon="Stop" Click="{x:Bind OnStopTimer}" />
</CommandBar>
</Page.TopAppBar>
The hand of the clock is defined using the shape Line . To rotate the
line, you use a RotateTransform element:
<Canvas Width="300" Height="300">
<Ellipse Width="10" Height="10" Fill="Red"
Canvas.Left="145"
Canvas.Top="145" />
<Line Canvas.Left="150" Canvas.Top="150" Fill="Green"
StrokeThickness="3"
Stroke="Blue" X1="0" Y1="0" X2="120" Y2="0" >
<Line.RenderTransform>
<RotateTransform CenterX="0" CenterY="0" Angle="270"
x:Name="rotate" />
</Line.RenderTransform>
</Line>
</Canvas>


**NOTE**
XAML shapes are explained in Chapter 35, “Styling XAML Apps.”
The DispatcherTimer object is created in the MainPage class. In the
constructor, the handler method is assigned to the Tick event, and the
Interval is specified to be one second. The timer is started in the
OnTimer method——the method that gets called when the user clicks the
Play button in the CommandBar (code file
WinAppTimer/MainPage.xaml.cs ):
private DispatcherTimer _timer = new DispatcherTimer();
public MainPage()
{
this.InitializeComponent();
_timer.Tick += OnTick;
_timer.Interval = TimeSpan.FromSeconds(1);
}
private void OnTimer()
{
_timer.Start();
}
private void OnTick(object sender, object e)
{
double newAngle = rotate.Angle + 6;
if (newAngle >= 360) newAngle = 0;
rotate.Angle = newAngle;
}
private void OnStopTimer()
{
_timer.Stop();
}
When you run the application, the clock hand is shown (see Figure 21-
1).


FIGURE 21-1
THREADING ISSUES
Programming with multiple threads is challenging. When starting
multiple threads that access the same data, you can get intermittent
problems that are hard to find. The problems are the same whether
you use tasks, Parallel LINQ, or the Parallel class. To avoid getting
into trouble, you must pay attention to synchronization issues and the
problems that can occur with multiple threads. This section covers two
in particular: race conditions and deadlocks.
**NOTE**


Before synchronizing custom collection classes with the
synchronization types shown here, you should also read Chapter
11, “Special Collections,” to learn about collections that are
already thread-safe: concurrent collections.
The sample code for the ThreadingIssues sample makes use of these
namespaces:
System.Diagnostics
System.Threading
System.Threading.Tasks
static System.Console
You can start the sample application ThreadingIssues with command-
line arguments to simulate either race conditions or deadlocks.
Race Conditions
A race condition can occur if two or more threads access the same
objects and access to the shared state is not synchronized. To
demonstrate a race condition, the following example defines the class
StateObject , with an int field and the method ChangeState . In the
implementation of ChangeState , the state variable is verified to
determine whether it contains 5; if it does, the value is incremented.
Trace.Assert is the next statement, which immediately verifies that
state now contains the value 6.
After incrementing by 1 a variable that contains the value 5, you might
assume that the variable now has the value 6; but this is not
necessarily the case. For example, if one thread has just completed the
if (_state == 5) statement, it might be preempted, with the
scheduler running another thread. The second thread now goes into
the if body and, because the state still has the value 5, the state is
incremented by 1 to 6. The first thread is then scheduled again, and in
the next statement the state is incremented to 7. This is when the race
condition occurs, and the assert message is shown (code file
ThreadingIssues/SampleTask.cs ):


public class StateObject
{
private int _state = 5;
public void ChangeState(int loop)
{
if (_state == 5)
{
_state++;
if (_state != 6)
{
Console.WriteLine($"Race condition occurred after
{loop} loops");
Trace.Fail("race condition");
}
}
_state = 5;
}
}
You can verify this by defining a method for a task. The method
RaceCondition of the class SampleTask gets a StateObject as a
parameter. Inside an endless while loop, the ChangeState method is
invoked. The variable i is used just to show the loop number in the
assert message:
public class SampleTask
{
public void RaceCondition(object o)
{
Trace.Assert(o is StateObject, "o must be of type
StateObject");
StateObject state = o as StateObject;
int i = 0;
while (true)
{
state.ChangeState(i++);
}
}
}
In the Main method of the program, a new StateObject is created that
is shared among all the tasks. Task objects are created by invoking the
RaceCondition method with the lambda expression that is passed to
the Run method of the Task . The main thread then waits for user input.
However, there’s a good chance that the program will halt before


reading user input because a race condition will happen:
public void RaceConditions()
{
var state = new StateObject();
for (int i = 0; i < 2; i++)
{
Task.Run(() => new SampleTask().RaceCondition(state));
}
}
When you start the program, you get race conditions. How long it
takes until the first race condition happens depends on your system
and whether you build the program as a release build or a debug build.
With a release build, the problem happens more often because the
code is optimized. If you have multiple CPUs in your system or dual-
/quad-core CPUs, where multiple threads can run concurrently, the
problem also occurs more often than with a single-core CPU. The
problem occurs with a single-core CPU because thread scheduling is
preemptive, but the problem doesn’t occur that often.
In one run of the program on my system, I saw an error after 85,232
loops; in another run, the error manifested after 70,037 loops. If you
start the application multiple times, you always get different results.
You can avoid the problem by locking the shared object. You do this
inside the thread by locking the variable state, which is shared among
the threads, with the lock statement, as shown in the following
example. Only one thread can exist inside the lock block for the state
object. Because this object is shared among all threads, a thread must
wait at the lock if another thread has the lock for state. As soon as the
lock is accepted, the thread owns the lock and gives it up at the end of
the lock block. If every thread changing the object referenced with the
state variable is using a lock, the race condition no longer occurs:
public class SampleTask
{
public void RaceCondition(object o)
{
Trace.Assert(o is StateObject, "o must be of type
StateObject");
StateObject state = o as StateObject;
int i = 0;


while (true)
{
lock (state) // no race condition with this lock
{
state.ChangeState(i++);
}
}
}
}
**NOTE**
With the downloaded sample code, you need to uncomment the
lock statements for solving the issues with race conditions.
Instead of performing the lock when using the shared object, you can
make the shared object thread-safe. In the following code, the
ChangeState method contains a lock statement. Because you cannot
lock the state variable itself (only reference types can be used for a
lock), the variable sync of type object is defined and used with the lock
statement. If a lock is done using the same synchronization object
every time the value state is changed, race conditions no longer
happen:
public class StateObject
{
private int _state = 5;
private _object sync = new object();
public void ChangeState(int loop)
{
lock (_sync)
{
if (_state == 5)
{
_state++;
if (_state != 6)
{
Console.WriteLine($"Race condition occured after
{loop} loops");
Trace.Fail($"race condition at {loop}");
}


}
_state = 5;
}
}
}
Deadlocks
Too much locking can get you in trouble as well. In a deadlock, at least
two threads halt and wait for each other to release a lock. As both
threads wait for each other, a deadlock occurs, and the threads wait
endlessly.
To demonstrate deadlocks, the following code instantiates two objects
of type StateObject and passes them with the constructor of the
SampleTask class. Two tasks are created: one task running the method
Deadlock1 and the other task running the method Deadlock2 (code file
ThreadingIssues/Program.cs ):
var
var
new
new
state1 =
state2 =
Task(new
Task(new
new StateObject();
new StateObject();
SampleTask(state1, state2).Deadlock1).Start();
SampleTask(state1, state2).Deadlock2).Start();
The methods Deadlock1 and Deadlock2 now change the state of two
objects: s1 and s2 . That’s why two locks are generated. Deadlock1 first
does a lock for s1 and next for s2 . Deadlock2 first does a lock for s2 and
then for s1 . Now, occasionally the lock for s1 in Deadlock1 is resolved.
Next, a thread switch occurs, and Deadlock2 starts to run and gets the
lock for s2 . The second thread now waits for the lock of s1 . Because it
needs to wait, the thread scheduler schedules the first thread again,
which now waits for s2 . Both threads now wait and don’t release the
lock as long as the lock block is not ended. This is a typical deadlock
(code file ThreadingIssues/SampleTask.cs ):
public class SampleTask
{
public SampleTask(StateObject s1, StateObject s2)
{
_s1 = s1;
_s2 = s2;
}
private StateObject _s1;


private StateObject _s2;
public void Deadlock1()
{
int i = 0;
while (true)
{
lock (_s1)
{
lock (_s2)
{
_s1.ChangeState(i);
_s2.ChangeState(i++);
Console.WriteLine($"still running, {i}");
}
}
}
}
public void Deadlock2()
{
int i = 0;
while (true)
{
lock (_s2)
{
lock (_s1)
{
_s1.ChangeState(i);
_s2.ChangeState(i++);
Console.WriteLine($"still running, {i}");
}
}
}
}
}
As a result, the program runs some loops and soon becomes
unresponsive. The message “still running” is written a few times to the
console. Again, how soon the problem occurs depends on your system
configuration, and the result will vary.
A deadlock problem is not always as obvious as it is here. One thread
locks _ s1 and then _ s2 ; the other thread locks _ s2 and then _ s1 . In
this case, you just need to change the order so that both threads
perform the locks in the same order. In a bigger application, the locks


might be hidden deeply inside a method. You can prevent this problem
by designing a good lock order in the initial architecture of the
application, and by defining timeouts for the locks, as demonstrated in
the next section.
THE LOCK STATEMENT AND THREAD SAFETY
C# has its own keyword for the synchronization of multiple threads:
the lock statement. The lock statement provides an easy way to hold
and release a lock. Before adding lock statements, however, let’s look
at another race condition. The class SharedState demonstrates using
shared state between threads and shares an integer value (code file
SynchronizationSamples/SharedState.cs ):
public class SharedState
{
public int State { get; set; }
}
The sample code for all the following synchronization samples uses the
following namespaces:
System
System.Collections.Generic
System.Linq
System.Text
System.Threading
System.Threading.Tasks
The class Job contains the method DoTheJob , which is the entry point
for a new task. With the implementation, the State of the SharedState
object is incremented 50,000 times. The variable sharedState is
initialized in the constructor of this class (code file
SynchronizationSamples/Job.cs ):
public class Job
{
private SharedState _sharedState;


public Job(SharedState sharedState)
{
_sharedState = sharedState;
}
public void DoTheJob()
{
for (int i = 0; i < 50000; i++)
{
_sharedState.State += 1;
}
}
}
In the Main method, a SharedState object is created and passed to the
constructor of 20 Task objects. All tasks are started. After starting the
tasks, the Main method waits until every one of the 20 tasks is
completed. After the tasks are completed, the summarized value of the
shared state is written to the console. With 50,000 loops and 20 tasks,
a value of 1,000,000 could be expected. Often, however, this is not the
case (code file SynchronizationSamples/Program.cs ):
class Program
{
static void Main()
{
int numTasks = 20;
var state = new SharedState();
var tasks = new Task[numTasks];
for (int i = 0; i < numTasks; i++)
{
tasks[i] = Task.Run(() => new Job(state).DoTheJob());
}
Task.WaitAll(tasks);
Console.WriteLine($"summarized {state.State}");
}
}
The results of multiple runs of the application are as follows:
summarized
summarized
summarized
summarized
summarized












The behavior is different every time, but none of the results are
correct. As noted earlier, you will see big differences between debug
and release builds, and the type of CPU that you are using also affects
results. If you change the loop count to smaller values, you often will
get correct values——but not every time. In this case the application is
small enough to see the problem easily; in a large application, the
reason for such a problem can be hard to find.
You must add synchronization to this program. To do so, use the lock
keyword. Defining the object with the lock statement means that you
wait to get the lock for the specified object. You can pass only a
reference type. Locking a value type would just lock a copy, which
wouldn’t make any sense. In any case, the C# compiler issues an error
if value types are used with the lock statement. As soon as the lock is
granted——only one thread gets the lock——the block of the lock
statement can run. At the end of the lock statement block, the lock for
the object is released, and another thread waiting for the lock can be
granted access to it:
lock (obj)
{
// synchronized region
}
To lock static members, you can place the lock on the type object or a
static member:
lock (typeof(StaticClass))
{
}
You can make the instance members of a class thread-safe by using the
lock keyword. This way, only one thread at a time can access the
methods DoThis and DoThat for the same instance:
public class Demo
{
public void DoThis()
{
lock (this)
{
// only one thread at a time can access the DoThis and


DoThat methods
}
}
public void DoThat()
{
lock (this)
{
}
}
}
However, because the object of the instance can also be used for
synchronized access from the outside, and you can’t control this from
the class itself, you can apply the SyncRoot pattern. With the SyncRoot
pattern, a private object named _ syncRoot is created, and this object is
used with the lock statements:
public class Demo
{
private object _syncRoot = new object();
public void DoThis()
{
lock (_syncRoot)
{
// only one thread at a time can access the DoThis and
DoThat methods
}
}
public void DoThat()
{
lock (_syncRoot)
{
}
}
}
Using locks costs time and is not always necessary. You can create two
versions of a class: synchronized and unsynchronized. This is
demonstrated in the next example code by changing the class Demo .
The class Demo is not synchronized, as shown in the implementation of
the DoThis and DoThat methods. The class also defines the
IsSynchronized property, whereby the client can get information about
the synchronization option of the class. To make a synchronized


variant of the Demo class, you use the static method Synchronized to
pass an unsynchronized object, and this method returns an object of
type SynchronizedDemo . SynchronizedDemo is implemented as an inner
class that is derived from the base class Demo and overrides the virtual
members of the base class. The overridden members make use of the
SyncRoot pattern:
public class Demo
{
private class SynchronizedDemo: Demo
{
private object _syncRoot = new object();
private Demo _d;
public SynchronizedDemo(Demo d)
{
_d = d;
}
public override bool IsSynchronized => true;
public override void DoThis()
{
lock (_syncRoot)
{
_d.DoThis();
}
}
public override void DoThat()
{
lock (_syncRoot)
{
_d.DoThat();
}
}
}
public virtual bool IsSynchronized => false;
public static Demo Synchronized(Demo d)
{
if (!d.IsSynchronized)
{
return new SynchronizedDemo(d);
}
return d;


}
public virtual void DoThis()
{
}
public virtual void DoThat()
{
}
}
Bear in mind that when you use the SynchronizedDemo class, only
methods are synchronized. There is no synchronization for invoking
other members of this class.
Now, change the SharedState class that was not synchronized at first
to use the SyncRoot pattern. If you try to make the SharedState class
thread-safe by locking access to the properties with the SyncRoot
pattern, you still get the race condition shown earlier in the “Race
Conditions” section:
public class SharedState
{
private int _state = 0;
private object _syncRoot = new object();
public int State // there's still a race condition,
// don't do this!
{
get { lock (_syncRoot) { return _state; }}
set { lock (_syncRoot) { _state = value; }}
}
}
The thread that invokes the DoTheJob method is accessing the get
accessor of the SharedState class to get the current value of the state,
and then the set accessor sets the new value for the state. In between
calling the get and set accessors, the object is not locked, and another
thread can read the interim value (code file
SynchronizationSamples/Job.cs ):
public void DoTheJob()
{
for (int i = 0; i < 50000; i++)
{
_sharedState.State += 1;


}
}
Therefore, it is better to leave the SharedState class as it was earlier,
without thread safety (code file
SynchronizationSamples/SharedState.cs ):
public class SharedState
{
public int State { get; set; }
}
In addition, add the lock statement where it belongs, inside the
method DoTheJob (code file SynchronizationSamples/Job.cs ):
public void DoTheJob()
{
for (int i = 0; i < 50000; i++)
{
lock (_sharedState)
{
_sharedState.State += 1;
}
}
}
This way, the results of the application are always as expected.
**NOTE**
Using the lock statement in one place does not mean that all other
threads accessing the object are waiting. You have to explicitly
use synchronization with every thread accessing the shared state.
Of course, you can also change the design of the SharedState class and
offer incrementing as an atomic operation. This is a design question——
what should be an atomic functionality of the class? The next code
snippet just keeps the increment locked:
public class SharedState
{


private int _state = 0;
private object _syncRoot = new object();
public int State => _state;
public int IncrementState()
{
lock (_syncRoot)
{
return ++_state;
}
}
}
There is, however, a faster way to lock the increment of the state, as
shown in the next section.
INTERLOCKED
The Interlocked class is used to make simple statements for variables
atomic. i ++ is not thread-safe. It consists of getting a value from the
memory, incrementing the value by 1 , and storing the value back in
memory. These operations can be interrupted by the thread scheduler.
The Interlocked class provides methods for incrementing,
decrementing, exchanging, and reading values in a thread-safe
manner.
Using the Interlocked class is much faster than other synchronization
techniques. However, you can use it only for simple synchronization
issues.
For example, instead of using the lock statement to lock access to the
variable someState when setting it to a new value, in case it is null, you
can use the Interlocked class, which is faster:
lock (this)
{
if (_someState == null)
{
_someState = newState;
}
}
The faster version with the same functionality uses the
Interlocked.CompareExchange method.


Instead of performing incrementing inside a lock statement as shown
here:
public int State
{
get
{
lock (this)
{
return ++_state;
}
}
}
you can use Interlocked.Increment , which is faster:
public int State
{
get => Interlocked.Increment(ref _state);
}
MONITOR
The C# compiler resolves the lock statement to use the Monitor class.
The following lock statement
lock (obj)
{
// synchronized region for obj
}
is resolved to invoke the Enter method, which waits until the thread
gets the lock of the object. Only one thread at a time may be the owner
of the object lock. As soon as the lock is resolved, the thread can enter
the synchronized section. The Exit method of the Monitor class
releases the lock. The compiler puts the Exit method into a finally
handler of a try block so that the lock is also released if an exception is
thrown:
Monitor.Enter(obj);
try
{
// synchronized region for obj
}


finally
{
Monitor.Exit(obj);
}
**NOTE**
Chapter 14, “Errors and Exceptions,” covers the try / finally block.
The Monitor class has a big advantage over the lock statement of C#:
You can add a timeout value for waiting to get the lock. Therefore,
instead of endlessly waiting to get the lock, you can use the TryEnter
method shown in the following example, passing a timeout value that
defines the maximum amount of time to wait for the lock. If the lock
for obj is acquired, TryEnter sets the Boolean ref parameter to true
and performs synchronized access to the state guarded by the object
obj . If obj is locked for more than 500 milliseconds by another thread,
TryEnter sets the variable lockTaken to false , and the thread does not
wait any longer but is used to do something else. Maybe later, the
thread can try to acquire the lock again.
bool _lockTaken = false;
Monitor.TryEnter(_obj, 500, ref _lockTaken);
if (_lockTaken)
{
try
{
// acquired the lock
// synchronized region for obj
}
finally
{
Monitor.Exit(obj);
}
}
else
{
// didn't get the lock, do something else
}


SPINLOCK
If the overhead on object-based lock objects ( Monitor ) would be too
high because of garbage collection, you can use the SpinLock struct.
SpinLock is useful if you have many locks (for example, for every node
in a list) and hold times are always extremely short. You should avoid
holding more than one SpinLock , and don’t call anything that might
block.
Other than the architectural differences, SpinLock is very similar in
usage to the Monitor class. You acquire the lock with Enter or TryEnter
and release the lock with Exit . SpinLock also offers two properties to
provide information about whether it is currently locked: IsHeld and
IsHeldByCurrentThread .
**NOTE**
Be careful when passing SpinLock instances around. Because
SpinLock is defined as a struct , assigning one variable to another
creates a copy. Always pass SpinLock instances by reference.
WAITHANDLE
is an abstract base class that you can use to wait for a signal
to be set. You can wait for different things, because WaitHandle is a
base class and some classes are derived from it.
WaitHandle
With WaitHandle , you can wait for one signal to occur ( WaitOne ),
multiple objects that all must be signaled ( WaitAll ), or one of multiple
objects ( WaitAny ). WaitAll and WaitAny are static members of the
WaitHandle class and accept an array of WaitHandle parameters.
has a SafeWaitHandle property with which you can assign a
native handle to an operating system resource and wait for that
handle. For example, you can assign a SafeFileHandle to wait for a file
I/O operation to complete.
WaitHandle


The classes Mutex , EventWaitHandle , and Semaphore are derived from
the base class WaitHandle , so you can use any of these with waits.
MUTEX
(mutual exclusion) is one of the classes of the .NET Framework
that offers synchronization across multiple processes. It is very similar
to the Monitor class in that there is just one owner. That is, only one
thread can get a lock on the mutex and access the synchronized code
regions that are secured by the mutex.
Mutex
With the constructor of the Mutex class, you can define whether the
mutex should initially be owned by the calling thread, define a name
for the mutex, and determine whether the mutex already exists. In the
following example, the third parameter is defined as an out parameter
to receive a Boolean value if the mutex was newly created. If the value
returned is false , the mutex was already defined. The mutex might be
defined in a different process, because a mutex with a name is known
to the operating system and is shared among different processes. If no
name is assigned to the mutex, the mutex is unnamed and not shared
among different processes.
bool createdNew;
var mutex = new Mutex(false, "ProCSharpMutex", out
createdNew);
To open an existing mutex, you can also use the method
Mutex.OpenExisting , which doesn’t require the same .NET privileges as
creating the mutex with the constructor.
Because the Mutex class derives from the base class WaitHandle , you can
do a WaitOne to acquire the mutex lock and be the owner of the mutex
during that time. The mutex is released by invoking the ReleaseMutex
method:
if (mutex.WaitOne())
{
try
{
// synchronized region
}


finally
{
mutex.ReleaseMutex();
}
}
else
{
// some problem happened while waiting
}
Because a named mutex is known system-wide, you can use it to keep
an application from being started twice. In the following console
application, the constructor of the Mutex object is invoked. Then it is
verified whether the mutex with the name SingletonAppMutex exists
already. If it does, the application exits (code file
SingletonUsingMutex/Program.cs ):
static void Main()
{
bool mutexCreated;
var mutex = new Mutex(false, "SingletonAppMutex", out
mutexCreated);
if (!mutexCreated)
{
Console.WriteLine("You can only start one instance of the
application.");
Console.WriteLine("Exiting.");
return;
}
Console.WriteLine("Application running");
Console.WriteLine("Press return to exit");
Console.ReadLine();
}
SEMAPHORE
A semaphore is very similar to a mutex, but unlike the mutex, the
semaphore can be used by multiple threads at once. A semaphore is a
counting mutex, meaning that with a semaphore you can define the
number of threads that are allowed to access the resource guarded by
the semaphore simultaneously. This is useful if you need to limit the
number of threads that can access the available resources. For
example, if a system has three physical I/O ports available, three


threads can access them simultaneously, but a fourth thread needs to
wait until the resource is released by one of the other threads.
.NET Core provides two classes with semaphore functionality:
Semaphore and SemaphoreSlim . Semaphore can be named, can use
system-wide resources, and allows synchronization between different
processes. SemaphoreSlim is a lightweight version that is optimized for
shorter wait times.
In the following example application, in the Main method six tasks are
created along with one semaphore with a count of 3. In the constructor
of the Semaphore class, you can define the count for the number of
locks that can be acquired with the semaphore (the second parameter)
and the number of locks that are free initially (the first parameter). If
the first parameter has a lower value than the second parameter, the
difference between the values defines the already allocated semaphore
count. As with the mutex, you can also assign a name to the
semaphore to share it among different processes. Here a
SemaphoreSlim object is created that can only be used within the
process. After the SemaphoreSlim object is created, six tasks are started,
and they all wait for the same semaphore (code file
SemaphoreSample/Program.cs ):
class Program
{
static void Main()
{
int taskCount = 6;
int semaphoreCount = 3;
var semaphore = new SemaphoreSlim(semaphoreCount,
semaphoreCount);
var tasks = new Task[taskCount];
for (int i = 0; i < taskCount; i++)
{
tasks[i] = Task.Run(() => TaskMain(semaphore));
}
Task.WaitAll(tasks);
Console.WriteLine("All tasks finished");
}
//...
In the task’s main method, TaskMain , the task does a Wait to lock the
semaphore. Remember that the semaphore has a count of 3, so three


tasks can acquire the lock. Task 4 must wait, and here the timeout of
600 milliseconds is defined as the maximum wait time. If the lock
cannot be acquired after the wait time has elapsed, the task writes a
message to the console and repeats the wait in a loop. As soon as the
lock is acquired, the thread writes a message to the console, sleeps for
some time, and releases the lock. Again, with the release of the lock it
is important that the resource be released in all cases. That’s why the
Release method of the SemaphoreSlim class is invoked in a finally
handler (code file SemaphoreSample/Program.cs ):
// ...
public static void TaskMain(SemaphoreSlim semaphore)
{
bool isCompleted = false;
while (!isCompleted)
{
if (semaphore.Wait(600))
{
try
{
Console.WriteLine($"Task {Task.CurrentId} locks the
semaphore");
Task.Delay(2000).Wait();
}
finally
{
Console.WriteLine($"Task {Task.CurrentId} releases
the semaphore");
semaphore.Release();
isCompleted = true;
}
}
else
{
Console.WriteLine($"Timeout for task {Task.CurrentId};
wait again");
}
}
}
When you run the application, you can indeed see that with four
threads, the lock is made immediately. The tasks with IDs 7, 8, and 9
must wait. The wait continues in the loop until one of the other
threads releases the semaphore:


Task 4 locks the semaphore
Task 5 locks the semaphore
Task 6 locks the semaphore
Timeout for task 7; wait again
Timeout for task 7; wait again
Timeout for task 8; wait again
Timeout for task 7; wait again
Timeout for task 8; wait again
Timeout for task 7; wait again
Timeout for task 9; wait again
Timeout for task 8; wait again
Task 5 releases the semaphore
Task 7 locks the semaphore
Task 6 releases the semaphore
Task 4 releases the semaphore
Task 8 locks the semaphore
Task 9 locks the semaphore
Task 8 releases the semaphore
Task 7 releases the semaphore
Task 9 releases the semaphore
All tasks finished
EVENTS
Like mutex and semaphore objects, events are also system-wide
synchronization resources. For using system events from managed
code, the .NET Framework offers the classes ManualResetEvent ,
AutoResetEvent , ManualResetEventSlim , and CountdownEvent in the
namespace System.Threading .
**NOTE**
The event keyword from C# that is covered in Chapter 8,
“Delegates, Lambdas, and Events,” has nothing to do with the
event classes from the namespace System.Threading ; the event
keyword is based on delegates. However, both event classes are
.NET wrappers to the system-wide native event resource for
synchronization.
You can use events to inform other tasks that some data is present,


that something is completed, and so on. An event can be signaled or
not signaled. A task can wait for the event to be in a signaled state with
the help of the WaitHandle class, discussed earlier.
A ManualResetEventSlim is signaled by invoking the Set method, and
it’s returned to a nonsignaled state with the Reset method. If multiple
threads are waiting for an event to be signaled and the Set method is
invoked, then all threads waiting are released. In addition, if a thread
invokes the WaitOne method but the event is already signaled, the
waiting thread can continue immediately.
An AutoResetEvent is also signaled by invoking the Set method, and
you can set it back to a nonsignaled state with the Reset method.
However, if a thread is waiting for an auto-reset event to be signaled,
the event is automatically changed into a nonsignaled state when the
wait state of the first thread is finished. This way, if multiple threads
are waiting for the event to be set, only one thread is released from its
wait state. It is not the thread that has been waiting the longest for the
event to be signaled, but the thread waiting with the highest priority.
To demonstrate events with the ManualResetEventSlim class, the
following class Calculator defines the method Calculation , which is
the entry point for a task. With this method, the task receives input
data for calculation and writes the result to the variable result that can
be accessed from the Result property. As soon as the result is
completed (after a random amount of time), the event is signaled by
invoking the Set method of the ManualResetEventSlim (code file
EventSample/Calculator.cs ):
public class Calculator
{
private ManualResetEventSlim _mEvent;
public int Result { get; private set; }
public Calculator(ManualResetEventSlim ev)
{
_mEvent = ev;
}
public void Calculation(int x, int y)
{
Console.WriteLine($"Task {Task.CurrentId} starts


calculation");
Task.Delay(new Random().Next(3000)).Wait();
Result = x + y;
// signal the event-completed!
Console.WriteLine($"Task {Task.CurrentId} is ready");
_mEvent.Set();
}
}
The Main method of the program defines arrays of four
ManualResetEventSlim objects and four Calculator objects. Every
Calculator is initialized in the constructor with a
ManualResetEventSlim object, so every task gets its own event object to
signal when it is completed. Now, the Task class is used to enable
different tasks to run the calculation (code file
EventSample/Program.cs ):
class Program
{
static void Main()
{
const int taskCount = 4;
var mEvents = new ManualResetEventSlim[taskCount];
var waitHandles = new WaitHandle[taskCount];
var calcs = new Calculator[taskCount];
for (int i = 0; i < taskCount; i++)
{
int i1 = i;
mEvents[i] = new ManualResetEventSlim(false);
waitHandles[i] = mEvents[i].WaitHandle;
calcs[i] = new Calculator(mEvents[i]);
Task.Run(() => calcs[i1].Calculation(i1 + 1, i1 + 3));
}
//...
The WaitHandle class is now used to wait for any one of the events in
the array. WaitAny waits until any one of the events is signaled. In
contrast to ManualResetEvent , ManualResetEventSlim does not derive
from WaitHandle . That’s why a separate collection of WaitHandle objects
is kept, which is filled from the WaitHandle property of the
ManualResetEventSlim class. WaitAny returns an index value that
provides information about the event that was signaled. The returned
value matches the index of the WaitHandle array that is passed to


WaitAny .
Using this index, information from the signaled event can be
read:
for (int i = 0; i < taskCount; i++)
{
int index = WaitHandle.WaitAny(waitHandles);
if (index == WaitHandle.WaitTimeout)
{
Console.WriteLine("Timeout!!");
}
else
{
mEvents[index].Reset();
Console.WriteLine($"finished task for {index}, result:
{calcs[index].Result}");
}
}
When starting the application, you can see the tasks doing the
calculation and setting the event to inform the main thread that it can
read the result. At random times, depending on whether the build is a
debug or release build and on your hardware, you might see different
orders and a different number of tasks performing calls:
Task 4 starts calculation
Task 5 starts calculation
Task 6 starts calculation
Task 7 starts calculation
Task 7 is ready
finished task for 3, result:
Task 4 is ready
finished task for 0, result:
Task 6 is ready
finished task for 1, result:
Task 5 is ready
finished task for 2, result:








In a scenario like this, to fork some work into multiple tasks and later
join the result, the new CountdownEvent class can be very useful.
Instead of creating a separate event object for every task, you need to
create only one. CountdownEvent defines an initial number for all the
tasks that set the event, and after the count is reached, the
CountdownEvent is signaled.
The Calculator class is modified to use the CountdownEvent instead of


the ManualResetEvent . Rather than set the signal with the Set method,
CountdownEvent defines the Signal method (code file
EventSampleWithCountdownEvent/Calculator.cs ):
public class Calculator
{
private CountdownEvent _cEvent;
public int Result { get; private set; }
public Calculator(CountdownEvent ev)
{
_cEvent = ev;
}
public void Calculation(int x, int y)
{
Console.WriteLine($"Task {Task.CurrentId} starts
calculation");
Task.Delay(new Random().Next(3000)).Wait();
Result = x + y;
// signal the event-completed!
Console.WriteLine($"Task {Task.CurrentId} is ready");
_cEvent.Signal();
}
}
You can now simplify the Main method so that it’s only necessary to
wait for the single event. If you don’t deal with the results separately as
it was done before, this new edition might be all that’s needed:
const int taskCount = 4;
var cEvent = new CountdownEvent(taskCount);
var calcs = new Calculator[taskCount];
for (int i = 0; i < taskCount; i++)
{
calcs[i] = new Calculator(cEvent);
int i1 = i;
Task.Run(() => calcs[i1].Calculation, Tuple.Create(i1 + 1,
i1 + 3));
}
cEvent.Wait();
Console.WriteLine("all finished");
for (int i = 0; i < taskCount; i++)
{
Console.WriteLine($"task for {i}, result:


{calcs[i].Result}");
}
BARRIER
For synchronization, the Barrier class is great for scenarios in which
work is forked into multiple tasks and the work must be joined
afterward. Barrier is used for participants that need to be
synchronized. While the job is active, you can dynamically add
participants——for example, child tasks that are created from a parent
task. Participants can wait until the work is done by all the other
participants before continuing.
The BarrierSample is somewhat complex, but it’s worthwhile to
demonstrate the features of the Barrier type. The sample creates
multiple collections of two million random strings. Multiple tasks are
used to iterate through the collection and count the number of strings,
starting with a, b, c, and so on. The work is not only distributed
between different tasks, but also within a task. After all tasks are
iterated through the first collection of strings, the result is
summarized, and the tasks continue later with the next collection.
The method FillData creates a collection and fills it with random
strings (code file BarrierSample/Program.cs ):
public static IEnumerable<string> FillData(int size)
{
var r = new Random();
return Enumerable.Range(0, size).Select(x => GetString(r));
}
private static string GetString(Random r)
{
var sb = new StringBuilder(6);
for (int i = 0; i < 6; i++)
{
sb.Append((char)(r.Next(26) + 97));
}
return sb.ToString();
}
A helper method to show information about a Barrier is defined with
the method LogBarrierInformation :


private static void LogBarrierInformation(string info,
Barrier barrier)
{
Console.WriteLine($"Task {Task.CurrentId}: {info}. " +
$"{barrier.ParticipantCount} current and " +
$"{barrier.ParticipantsRemaining} remaining participants,
" +
$"phase {barrier.CurrentPhaseNumber}");
}
The CalculationInTask method defines the job performed by a task.
With the parameters, the third parameter references the Barrier
instance. The data that is used for the calculation is an array of
IList<string> . The last parameter, a jagged int array, will be used to
write the results as the task progresses.
The task makes the processing in a loop. With every loop, an array
element of IList<string>[] is processed. After every loop is
completed, the Task signals that it’s ready by invoking the
SignalAndWait method, and it waits until all the other tasks are ready
with this processing as well. This loop continues until the task is fully
finished. Then the task removes itself from the barrier by invoking the
method RemoveParticipant (code file BarrierSample/Program.cs ):
private static void CalculationInTask(int jobNumber, int
partitionSize,
Barrier barrier, IList<string>[] coll, int loops, int[][]
results)
{
LogBarrierInformation("CalculationInTask started",
barrier);
for (int i = 0; i < loops; i++)
{
var data = new List<string>(coll[i]);
int start = jobNumber * partitionSize;
int end = start + partitionSize;
Console.WriteLine($"Task {Task.CurrentId} in loop {i}:
partition " +
$"from {start} to {end}");
for (int j = start; j < end; j++)
{
char c = data[j][0];
results[i][c - 97]++;
}
Console.WriteLine($"Calculation completed from task


{Task.CurrentId} " +
$"in loop {i}. {results[i][0]} times a, {results[i]
[25]} times z");
LogBarrierInformation("sending signal and wait for all",
barrier);
barrier.SignalAndWait();
LogBarrierInformation("waiting completed", barrier);
}
barrier.RemoveParticipant();
LogBarrierInformation("finished task, removed participant",
barrier);
}
With the Main method, a Barrier instance is created. In the
constructor, you can specify the number of participants. In the
example, this number is 3 ( numberTasks + 1 ) because there are two
created tasks, and the Main method is a participant as well. Using
Task.Run , two tasks are created to fork the iteration through the
collection into two parts. After starting the tasks, using SignalAndWait ,
the main method signals its completion and waits until all remaining
participants either signal their completion or remove themselves as
participants from the barrier. As soon as all participants are ready with
one iteration, the results from the tasks are zipped together with the
Zip extension method. Then the next iteration is done to wait for the
next results from the tasks (code file BarrierSample/Program.cs ):
static void Main()
{
const int numberTasks = 2;
const int partitionSize = 1000000;
const int loops = 5;
var taskResults = new Dictionary<int, int[][]>();
var data = new List<string>[loops];
for (int i = 0; i < loops; i++)
{
data[i] = new List<string>(FillData(partitionSize *
numberTasks);
}
var barrier = new Barrier(numberTasks + 1);
LogBarrierInformation("initial participants in barrier",
barrier);
for (int i = 0; i < numberTasks; i++)
{


barrier.AddParticipant();
int jobNumber = i;
taskResults.Add(i, new int[loops][]);
for (int loop = 0; loop < loops; loop++)
{
taskResult[i, loop] = new int[26];
}
Console.WriteLine("Main - starting task job
{jobNumber}");
Task.Run(() => CalculationInTask(jobNumber,
partitionSize,
barrier, data, loops, taskResults[jobNumber]));
}
for (int loop = 0; loop < 5; loop++)
{
LogBarrierInformation("main task, start signaling and
wait", barrier);
barrier.SignalAndWait();
LogBarrierInformation("main task waiting completed",
barrier);
int[][] resultCollection1 = taskResults[0];
int[][] resultCollection2 = taskResults[1];
var resultCollection = resultCollection1[loop].Zip(
resultCollection2[loop], (c1, c2) => c1 + c2);
char ch = 'a';
int sum = 0;
foreach (var x in resultCollection)
{
Console.WriteLine($"{ch++}, count: {x}");
sum += x;
}
LogBarrierInformation($"main task finished loop {loop},
sum: {sum}",
barrier);
}
Console.WriteLine("finished all iterations");
Console.ReadLine();
}
**NOTE**
Jagged arrays are explained in Chapter 7, “Arrays.” The Zip


extension method is explained in Chapter 12, “Language
Integrated Query.”
When you run the application, you can see output like the following. In
the output you can see that every call to AddParticipant increases the
participant count as well as the remaining participant count. As soon
as one participant invokes SignalAndWait , the remaining participant
count is decremented. When the remaining participant count reaches
0, the wait of all participants ends, and the next phase begins:
Task : initial participants in barrier. 1 current and 1
remaining participants,
phase 0.
Main - starting task job 0
Main - starting task job 1
Task : main task, starting signaling and wait. 3 current and
3 remaining participants, phase 0.
Task 4: CalculationInTask started. 3 current and 2 remaining
participants, phase 0.
Task 5: CalculationInTask started. 3 current and 2 remaining
participants, phase 0.
Task 4 in loop 0: partition from 0 to 1000000
Task 5 in loop 0: partition from 1000000 to 2000000
Calculation completed from task 4 in loop 0. 38272 times a,
38637 times z
Task 4: sending signal and wait for all. 3 current and
2 remaining participants, phase 0.
Calculation completed from task 5 in loop 0. 38486 times a,
38781 times z
Task 5: sending signal and wait for all. 3 current and
1 remaining participants, phase 0.
Task 5: waiting completed. 3 current and 3 remaining
participants, phase 1
Task 4: waiting completed. 3 current and 3 remaining
participants, phase 1
Task : main waiting completed. 3 current and 3 remaining
participants, phase 1
READERWRITERLOCKSLIM
For a locking mechanism to allow multiple readers but only one writer
for a resource, you can use the class ReaderWriterLockSlim . This class
offers a locking functionality in which multiple readers can access the


resource if no writer locked it, and only a single writer can lock the
resource.
The ReaderWriterLockSlim class has blocking and nonblocking
methods to acquire a read lock, such as EnterReadLock (blocking) and
TryEnterReadLock (nonblocking), and to acquire a write lock with
EnterWriteLock (blocking) and TryEnterWriteLock (nonblocking). If a
task reads first and writes afterward, it can acquire an upgradable read
lock with EnterUpgradableReadLock or TryEnterUpgradableReadLock .
With this lock, the write lock can be acquired without releasing the
read lock.
Several properties of this class offer information about the held locks,
such as CurrentReadCount , WaitingReadCount ,
WaitingUpgradableReadCount , and WaitingWriteCount .
The following example creates a collection containing six items and a
ReaderWriterLockSlim object. The method ReaderMethod acquires a
read lock to read all items of the list and write them to the console.
The method WriterMethod tries to acquire a write lock to change all
values of the collection. In the Main method, six tasks are started that
invoke either the method ReaderMethod or the method WriterMethod
(code file ReaderWriterLockSample/Program.cs ):
class Program
{
private static List<int> _items = new List<int>() { 0, 1,
2, 3, 4, 5};
private static ReaderWriterLockSlim _rwl =
new
ReaderWriterLockSlim(LockRecursionPolicy.SupportsRecursion);
public static void ReaderMethod(object reader)
{
try
{
_rwl.EnterReadLock();
for (int i = 0; i < _items.Count; i++)
{
Console.WriteLine($"reader {reader}, loop: {i}, item:
{_items[i]}");
Task.Delay(40).Wait();
}


}
finally
{
_rwl.ExitReadLock();
}
}
public static void WriterMethod(object writer)
{
try
{
while (!_rwl.TryEnterWriteLock(50))
{
Console.WriteLine($"Writer {writer} waiting for the
write lock");
Console.WriteLine($"current reader count:
{_rwl.CurrentReadCount}");
}
Console.WriteLine($"Writer {writer} acquired the
lock");
for (int i = 0; i < _items.Count; i++)
{
_items[i]++;
Task.Delay(50).Wait();
}
Console.WriteLine($"Writer {writer} finished");
}
finally
{
_rwl.ExitWriteLock();
}
}
static void Main()
{
var taskFactory = new
TaskFactory(TaskCreationOptions.LongRunning,
TaskContinuationOptions.None);
var tasks = new Task[6];
tasks[0] = taskFactory.StartNew(WriterMethod,
tasks[1] = taskFactory.StartNew(ReaderMethod,
tasks[2] = taskFactory.StartNew(ReaderMethod,
tasks[3] = taskFactory.StartNew(WriterMethod,
tasks[4] = taskFactory.StartNew(ReaderMethod,
tasks[5] = taskFactory.StartNew(ReaderMethod,
Task.WaitAll(tasks);
}
}


1);
1);
2);
2);
3);
4);
When you run the application, the following shows that the first writer
gets the lock first. The second writer and all readers need to wait. Next,
the readers can work concurrently, while the second writer still waits
for the resource:
Writer 1 acquired the lock
Writer 2 waiting for the write
current reader count: 0
Writer 2 waiting for the write
current reader count: 0
Writer 2 waiting for the write
current reader count: 0
Writer 2 waiting for the write
current reader count: 0
Writer 1 finished
reader 4, loop: 0, item: 1
reader 1, loop: 0, item: 1
Writer 2 waiting for the write
current reader count: 4
reader 2, loop: 0, item: 1
reader 3, loop: 0, item: 1
reader 4, loop: 1, item: 2
reader 1, loop: 1, item: 2
reader 3, loop: 1, item: 2
reader 2, loop: 1, item: 2
Writer 2 waiting for the write
current reader count: 4
reader 4, loop: 2, item: 3
reader 1, loop: 2, item: 3
reader 2, loop: 2, item: 3
reader 3, loop: 2, item: 3
Writer 2 waiting for the write
current reader count: 4
reader 4, loop: 3, item: 4
reader 1, loop: 3, item: 4
reader 2, loop: 3, item: 4
reader 3, loop: 3, item: 4
reader 4, loop: 4, item: 5
reader 1, loop: 4, item: 5
Writer 2 waiting for the write
current reader count: 4
reader 2, loop: 4, item: 5
reader 3, loop: 4, item: 5
reader 4, loop: 5, item: 6
reader 1, loop: 5, item: 6
reader 2, loop: 5, item: 6
lock
lock
lock
lock
lock
lock
lock
lock


reader 3, loop: 5, item: 6
Writer 2 waiting for the write lock
current reader count: 4
Writer 2 acquired the lock
Writer 2 finished
LOCKS WITH AWAIT
In case you try to use the lock keyword while having the async
keyword in the lock block, you get this compilation error: cannot
await in the body of a lock statement . The reason is that after the
async completes, the method might run in a different thread than
before the async keyword. The lock keyword needs to release the lock
in the same thread as the lock is acquired.
Such a code block results in compilation errors:
static async Task IncorrectLockAsync()
{
lock (s_syncLock)
{
Console.WriteLine($"{nameof(IncorrectLockAsync)}
started");
await Task.Delay(500); // compiler error: cannot await
in the body
// of a lock statement
Console.WriteLine($"{nameof(IncorrectLockAsync)}
ending");
}
}
How can this be solved? You cannot use a Monitor for this, as the
Monitor needs to release the lock from the same thread where it
entered the lock. The lock keyword is based on Monitor .
While the Mutex object can be used for synchronization across different
processes, it has the same issues: it grants a lock for a thread.
Releasing the lock from a different thread is not possible. Instead, you
can use the Semaphore ——or the SemaphoreSlim class. Semaphores can
release the semaphore from a different thread.
The following code snippet waits to acquire a semaphore using
WaitAsync on a SemaphoreSlim object. The SemaphoreSlim object is


initialized with a count of 1, thus the wait on the semaphore is only
granted once. In the finally code block, the semaphore is released by
invoking the Release method (code file LockAcrossAwait/Program.cs ):
private static SemaphoreSlim s_asyncLock = new
SemaphoreSlim(1);
static async Task LockWithSemaphore(string title)
{
Console.WriteLine($"{title} waiting for lock");
await s_asyncLock.WaitAsync();
try
{
Console.WriteLine($"{title} {nameof(LockWithSemaphore)}
started");
await Task.Delay(500);
Console.WriteLine($"{title} {nameof(LockWithSemaphore)}
ending");
}
finally
{
s_asyncLock.Release();
}
}
Let’s try to invoke this method from multiple tasks concurrently. The
method RunUseSemaphoreAsync starts six tasks to invoke the
LockWithSemaphore method concurrently:
static async Task RunUseSemaphoreAsync()
{
Console.WriteLine(nameof(RunUseSemaphoreAsync));
string[] messages = { "one", "two", "three", "four",
"five", "six" };
Task[] tasks = new Task[messages.Length];
for (int i = 0; i < messages.Length; i++)
{
string message = messages[i];
tasks[i] = Task.Run(async () =>
{
await LockWithSemaphore(message);
});
}
await Task.WhenAll(tasks);


Console.WriteLine();
}
Running the program, you can see that multiple tasks are started
concurrently, but after the semaphore is locked, all other tasks need to
wait until the semaphore is released again:
RunLockWithAwaitAsync
two waiting for lock
two LockWithSemaphore started
three waiting for lock
five waiting for lock
four waiting for lock
six waiting for lock
one waiting for lock
two LockWithSemaphore ending
three LockWithSemaphore started
three LockWithSemaphore ending
five LockWithSemaphore started
five LockWithSemaphore ending
four LockWithSemaphore started
four LockWithSemaphore ending
six LockWithSemaphore started
six LockWithSemaphore ending
one LockWithSemaphore started
one LockWithSemaphore ending
To make the use of the lock easier, you can create a class that
implements the IDisposable interface to manage the resource. With
this class, you can use the using statement in the same way as the lock
statement is used to lock and release the semaphore.
The following code snippet implements the AsyncSemaphore class that
allocates a SemaphoreSlim in the constructor, and on invoking the
WaitAsync method on the AsyncSemaphore , the inner class
SemaphoreReleaser is returned that implements the interface
IDisposable . On calling the Dispose method, the semaphore is released
(code file LockAcrossAwait/AsyncSemaphore.cs ):
public sealed class AsyncSemaphore
{
private class SemaphoreReleaser : IDisposable
{
private SemaphoreSlim _semaphore;


public SemaphoreReleaser(SemaphoreSlim semaphore) =>
_semaphore = semaphore;
public void Dispose() => _semaphore.Release();
}
private SemaphoreSlim _semaphore;
public AsyncSemaphore() =>
_semaphore = new SemaphoreSlim(1);
public async Task<IDisposable> WaitAsync()
{
await _semaphore.WaitAsync();
return new SemaphoreReleaser(_semaphore) as IDisposable;
}
}
Changing the implementation from the LockWithSemaphore method
shown previously, now a using statement can be used where the
semaphore is locked. Remember, the using statement creates a
catch / finally block, and in the finally block the Dispose method gets
invoked (code file LockAcrossAwait/Program.cs ):
private static AsyncSemaphore s_asyncSemaphore = new
AsyncSemaphore();
static async Task UseAsyncSemaphore(string title)
{
using (await s_asyncSemaphore.WaitAsync())
{
Console.WriteLine($"{title} {nameof(LockWithSemaphore)}
started");
await Task.Delay(500);
Console.WriteLine($"{title} {nameof(LockWithSemaphore)}
ending");
}
}
Using the UseAsyncSemaphore method similar to the LockWithSemaphore
method results in the same behavior. However, with a class written
once, locking across await becomes simpler.
SUMMARY
This chapter explored how to code applications that use multiple tasks


by using the System.Threading.Tasks namespace. Using
multithreading in your applications takes careful planning. Too many
threads can cause resource issues, and not enough threads can cause
your application to be sluggish and perform poorly. With tasks, you
get an abstraction to threads. This abstraction helps you avoid creating
too many threads because threads are reused from a pool.
You’ve seen various ways to create multiple tasks, such as the Parallel
class, which offers both task and data parallelism with
Parallel.Invoke , Parallel.ForEach , and Parallel.For . With the Task
class, you’ve seen how to gain more control over parallel
programming. Tasks can run synchronously in the calling thread,
using a thread from a thread pool, and a separate new thread can be
created. Tasks also offer a hierarchical model that enables the creation
of child tasks, also providing a way to cancel a complete hierarchy.
The cancellation framework offers a standard mechanism that can be
used in the same manner with different classes to cancel a task early.
You’ve seen several synchronization objects that are available with
.NET, and with which scenario what synchronization object has its
advantage. An easy synchronization can be done using the lock
keyword. Behind the scenes, it’s the Monitor type that allows setting
timeouts, which is not possible with the lock keyword. For
synchronization between processes, the Mutex object offers similar
functionality. With the Semaphore object you’ve seen a synchronization
object with a count——some tasks are allowed to run concurrently. To
inform others of information that is ready, various kinds of event
objects have been discussed, such as the AutoResetEvent ,
ManualResetEvent , and CountdownEvent . A straightforward way to have
multiple readers and one writer is offered by the ReaderWriterLock .
The Barrier type allows for more complex scenarios where multiple
tasks can run concurrently until a synchronization point is reached. As
soon as all tasks reach this point, all can continue concurrently to meet
at the next synchronization point.
Here are some final guidelines regarding threading:
Try to keep synchronization requirements to a minimum.
Synchronization is complex and blocks threads. You can avoid it if


you try to avoid sharing state. Of course, this is not always possible.
Static members of a class should be thread-safe. Usually, this is the
case with classes in the .NET Framework.
Instance state does not need to be thread-safe. For best
performance, synchronization is best used outside the class where
it is needed, and not with every member of the class. Instance
members of .NET Framework classes usually are not thread-safe.
In the Microsoft API documentation, you can find this information
documented for every class of the .NET Framework in the “Thread
Safety” section.
The next chapter gives information on another core .NET topic: files
and streams.